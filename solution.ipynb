{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ЗАГРУЖАЕМ ДАТАСЕТ**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first element: \n",
      "{'id': 0, 'quote': '«У ннас среди ночi в райооне 55 часов упала полxа с водой в сттеклянной  таре, тоесть, посреди этой лужа, сстеккла, всёё, это уже поод утро, кстати, ээто не перрвыи раз,,, они сами по себе падают, как-то неправилььно рассчитывают, мы же должны выставлять по определённой картинке, у нас при мне было уже пару раз, чтоо сами по себе грохаются эти полки с буттылками» \\n\\n\\n'}\n",
      "Types: \n",
      "list of dict elements\n"
     ]
    }
   ],
   "source": [
    "path = 'cintra_phoenix_oils_hr_mgck_feather.json'\n",
    "dataset = load_data(path)\n",
    "\n",
    "print(f'first element: \\n{dataset[0]}')\n",
    "print(f'Types: \\n{type(dataset).__name__} of {type(dataset[0]).__name__} elements')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ПРЕДОБРАБОТКА ДАННЫХ**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**На этапе предобработки возникает несколько сложностей и спорных моментов. Начну с этапов, которые не требуют особых пояснений:**\n",
    "\n",
    "1. Разбиваю предложения на токены (функция `tokenize`).\n",
    "\n",
    "2. Привожу токены к нижнему регистру (функция `to_lower`).\n",
    "\n",
    "3. Теперь немного контекста: в тексте встречается множество слов, в которых русские буквы заменены на их \"английские аналоги\", поэтому эмпирическим путем был создан словарь соответствий англ→рус (функция `en_ru_mapping`).\n",
    "\n",
    "4. Похожая проблема связана с заменой цифр на буквы, здесь есть некая закономерность, которую можно описать так (функция `replace_numbers`):\n",
    "    - '3' → 'З', если перед '3' стоит гласная (с '3' есть одно исключение, но оно не критично);\n",
    "    - '3' → 'Е', если перед '3' стоит согласная;\n",
    "    - '0' → 'О';\n",
    "    - '1' → 'И';\n",
    "    - '7' → 'Т'.\n",
    "    - Есть также замены с '4', но их очень мало, поэтому можно игнорировать.\n",
    "\n",
    "5. После замены всех потенциально информативных цифр и англоязычных букв на русские удаляю все символы, которые не являются русскими буквами.\n",
    "\n",
    "6. Удаляю повторяющиеся буквы (например, \"приииивет\"), из-за этого могут пострадать слова с удвоенными буквами, однако эта потеря влияет на качество датасета гораздо меньше, чем наличие произвольного количества повторений (функция `remove_repeating_letters`).\n",
    "\n",
    "7. Исправляю грамматические ошибки с помощью `Yandex.Speller`. Отмечу, что в тексте есть много ошибок, которые не удается исправить (подробнее об этом ниже). После `spell_correction` снова использую `tokenize`, так как внутри функции токены объединяются в строку для ускорения работы (ускорение в ~ x30).\n",
    "\n",
    "8. Стемминг (приведение слова к базовой форме), как по мне, здесь малоэффективен, поэтому было решено от него отказаться (подробнее о причинах ниже) (функция `stemming`).\n",
    "\n",
    "9. Лемматизация стандартизирует слова, приводя их к единой форме; считаю, что это эффективнее, чем стемминг. Пробовал разные комбинации: только стемминг, стемминг + лемматизация и только лемматизацию. Последний вариант оказался лучше (оценивал по получившимся тематикам) (функция `lemmatize`).\n",
    "\n",
    "10. Удаляю стоп-слова (функция `remove_stop_words`).\n",
    "\n",
    "---\n",
    "\n",
    "**ПРОБЛЕМЫ**\n",
    "\n",
    "В тексте множество грамматических ошибок, из-за чего эффективность стемминга и лемматизации снижается (не обрабатываются слова с ошибками → не стандартизируются нужные слова → не выполняется их основная функция), также не удаляются стоп-слова с ошибками, что добавляет тексту лишний шум.\n",
    "\n",
    "Для решения этой проблемы искал `Spell Corrector`, удовлетворяющий следующим требованиям:\n",
    "1. Поддержка русского языка;\n",
    "2. Высокая скорость работы и небольшой размер.\n",
    "\n",
    "В ходе поиска протестировал следующие инструменты:\n",
    "- `python-language-tools`: запускает локальный сервер на Java, запросы к нему работают медленно, не хватает памяти.\n",
    "- `JamSpell`: https://github.com/bakwc/JamSpell. Русская модель работает только под Linux (в Windows — через WSL). Можно обучить свою модель, которая будет работать в Windows. Быстр, но качество сомнительное: например, исправил слово \"томленый\" на \"атомный\".\n",
    "- `Yandex.Speller` через библиотеку `pyaspeller`: работает быстро, коверканий не замечено. Недостатки — подключение через `speller.yandex.net`, что требует стабильного соединения, и неспособность исправлять простые ошибки вроде \"полха\" → \"полка\".\n",
    "\n",
    "---\n",
    "\n",
    "**Что можно улучшить**\n",
    "\n",
    "1. Общая проблема всех вышеуказанных корректоров — исправление по корпусу, где нет слов из узкоспециализированной области (например, связанных с корпорацией или миром Ведьмака).\n",
    "\n",
    "    При наличии дополнительных ресурсов и времени лучшим вариантом было бы прогнать текст через Fine-tuned LLM, которая хорошо справляется с коррекцией ошибок и учитывает контекст. \n",
    "\n",
    "2. Извлечение именованных сущностей и их обработка. Можно реализовать с помощью `spacy`. К сожалению, вспомнил об этом только под конец.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Loassar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Loassar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from preprocessing import (\n",
    "    to_lower,\n",
    "    tokenize,\n",
    "    en_ru_mapping,\n",
    "    replace_numbers,\n",
    "    only_ru_chars,\n",
    "    remove_repeating_letters,\n",
    "    remove_stop_words,\n",
    "    spell_correction,\n",
    "    stemming,\n",
    "    lemmatize,\n",
    "    remove_freqs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(sentences):\n",
    "    processed_sentences = []\n",
    "    pipeline = [\n",
    "        tokenize,\n",
    "        to_lower,\n",
    "        en_ru_mapping,\n",
    "        replace_numbers,\n",
    "        only_ru_chars,\n",
    "        remove_repeating_letters,\n",
    "        # stemming,\n",
    "        spell_correction,\n",
    "        tokenize,\n",
    "        lemmatize,\n",
    "        remove_stop_words,\n",
    "    ]\n",
    "    for sentence in tqdm(sentences, desc=\"Sentence processing\"):\n",
    "        for step in pipeline:\n",
    "            sentence = step(sentence)\n",
    "        processed_sentences.append(sentence)\n",
    "\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentence processing: 100%|██████████| 959/959 [02:21<00:00,  6.79it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = [comment['quote'] for comment in dataset]\n",
    "processed_sentences = process_sentences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['среди', 'ночь', 'район', 'час', 'упасть', 'полха', 'вода', 'стекляна', 'тара', 'тобыть', 'посреди', 'лужа', 'стекло', 'всё', 'это', 'утро', 'кстати', 'это', 'первыя', 'падать', 'както', 'неправильно', 'расчитывать', 'должный', 'выставлять', 'определёный', 'картинка', 'пара', 'грохаться', 'полка', 'бутылка']\n",
      "['програм', 'повышение', 'квалификаци', 'гильдия', 'цмф', 'хитрый', 'хитрый', 'весь', 'хотеть', 'повышаться', 'свой', 'предпочтение', 'направление', 'гильдия', 'гильдия', 'толкать', 'свой', 'нужно', 'сотрудник', 'белый', 'змея', 'например', 'програм', 'гибкий', 'сотрудник', 'выбирать', 'нужно']\n",
      "['мур', 'новиград', 'город', 'контраст', 'цмф', 'змс', 'человек', 'превратиться', 'кот', 'всё', 'занятый', 'никто', 'хотеть', 'протянуть', 'лапка', 'товарищ', 'помочь', 'груз', 'всё', 'напряженить', 'спешка', 'мур', 'ранний', 'хороший', 'работать', 'втроём', 'порознь']\n",
      "['тип', 'полгода', 'ждать', 'установка', 'фильтр', 'маслобаза', 'думать', 'бесконечный', 'огненный', 'огонь', 'затягивать', 'поставка', 'давно', 'всё', 'порядок']\n",
      "['всё', 'просто', 'ранний', 'бумажка', 'летать', 'теряться', 'искать', 'приходиться', 'весь', 'зелёный', 'всё', 'место', 'плантире', 'весь', 'информация', 'рука', 'ничо', 'пропалаять', 'работать', 'стать', 'гораздо', 'удобный']\n"
     ]
    }
   ],
   "source": [
    "for s in processed_sentences[0:5]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sentences_without_freqs = remove_freqs(processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['среди ночь район час упасть полха вода стекляна тара тобыть посреди лужа стекло утро кстати первыя падать неправильно расчитывать должный выставлять определёный картинка пара грохаться полка бутылка',\n",
       " 'програм повышение квалификаци гильдия хитрый хитрый хотеть повышаться предпочтение направление гильдия гильдия толкать нужно сотрудник белый змея например програм гибкий сотрудник выбирать нужно',\n",
       " 'новиград город контраст превратиться кот занятый никто хотеть протянуть лапка товарищ помочь груз напряженить спешка ранний втроём порознь',\n",
       " 'полгода ждать установка фильтр маслобаза думать бесконечный огненный огонь затягивать поставка давно порядок',\n",
       " 'ранний бумажка летать теряться искать приходиться зелёный место плантире информация рука ничо пропалаять стать гораздо удобный']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentences_without_freqs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ТЕМАТИЧЕСКОЕ МОДЕЛИРОВАНИЕ**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Задачи:**\n",
    "- Определить количество тематик, встречающихся в тексте, и дать им описание;\n",
    "- Сопоставить каждый комментарий работника с набором тематик;\n",
    "- Разработать модель, определяющую тематику ранее неизвестных комментариев.\n",
    "\n",
    "**Решение:**\n",
    "Необходимо решить задачу тематического моделирования. Наиболее популярные варианты:\n",
    "\n",
    "1. `LDA`: \n",
    "    - Почему я решил не применять `LDA`?\n",
    "    - Для `LDA` необходимо заранее определить количество тем, что вызывает сложности. Существует несколько способов определения оптимального количества кластеров (в данном случае — тематик), но ни разу не удалось получить результат, отличный от двойки. Как мне кажется, это связано с тем, что тривиальный случай — разделение комментариев на положительные и отрицательные.\n",
    "    - Этот подход мог бы сработать при наличии эвристик. В данном случае такими эвристиками могли бы стать вопросы, задаваемые работникам, и идеи, заложенные в их основе (которые мог описать человек, их составлявший).\n",
    "\n",
    "2. `BERTopic` https://maartengr.github.io/BERTopic/index.html:\n",
    "    - Решение на основе трансформеров. Здесь идея проста (как палка и веревка): строим векторное представление (эмбеддинги) для каждого комментария, затем применяем метод кластеризации, после чего составляем описание для каждого кластера (название тематики).\n",
    "\n",
    "    Двигаемся по порядку:\n",
    "    1. Модель для построения эмбеддингов должна быть \"легкой\", быстрой, поддерживать русский язык (желательно с соответствующей специализацией) и создавать адекватные представления на малых данных (1–3 предложения). Я нашел лишь одну модель, которая популярна и соответствует всем моим требованиям — это `'cointegrated/rubert-tiny2'`.\n",
    "    2. По умолчанию в `BERTopic` для кластеризации используется `HDBSCAN`. В целом эта модель хорошо подходит для задачи \"слепой\" кластеризации, когда неизвестно ни количество, ни примерный состав кластеров, поэтому я оставил ее, лишь немного поигравшись с параметрами.\n",
    "    3. `UMAP` — это один из способов понижения размерности для эмбеддингов, используемый для оптимизации. Как правило, с ним у моделей кластеризации получается более качественный инференс, так как эти алгоритмы не слишком хорошо работают с данными высокой размерности.\n",
    "    4. `CountVectorizer` отвечает непосредственно за репрезентативную часть кластеров. Вся часть, касающаяся обработки текста, была вынесена в разделе предобработки, поэтому здесь можно изменить разве что n-граммы.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Loassar\\Documents\\Career\\GPN\\final\\gpn-ds-competition\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 21:46:47,868 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 30/30 [00:00<00:00, 89.33it/s]\n",
      "2024-11-14 21:46:48,217 - BERTopic - Embedding - Completed ✓\n",
      "2024-11-14 21:46:48,217 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-11-14 21:46:53,500 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-11-14 21:46:53,500 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-11-14 21:46:53,510 - BERTopic - Cluster - Completed ✓\n",
      "2024-11-14 21:46:53,520 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-11-14 21:46:53,604 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "sentence_model = SentenceTransformer('cointegrated/rubert-tiny2', device=device)\n",
    "\n",
    "umap_model = UMAP(n_neighbors=20, n_components=4, min_dist=0.1, metric='cosine')\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=20, min_samples=5, metric='euclidean', cluster_selection_method='leaf')\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=sentence_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    verbose=True,\n",
    "    language=\"russian\"\n",
    ")\n",
    "\n",
    "topics, probabilities = topic_model.fit_transform(processed_sentences_without_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>449</td>\n",
       "      <td>-1_новый_стать_время_например</td>\n",
       "      <td>[новый, стать, время, например, проблема, фени...</td>\n",
       "      <td>[цинтрийский феникс стараться обновление ценик...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0_скатя_снежный_зеркало_словно</td>\n",
       "      <td>[скатя, снежный, зеркало, словно, дракон, испо...</td>\n",
       "      <td>[мантикорный трубка дело хорош перекачка эфект...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>1_день_смена_ночной_делать</td>\n",
       "      <td>[день, смена, ночной, делать, график, стоить, ...</td>\n",
       "      <td>[растроить девушка буквально декабрь проситься...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>2_знать_образование_делать_опыт</td>\n",
       "      <td>[знать, образование, делать, опыт, идея, безоп...</td>\n",
       "      <td>[думать малый знание начинать знать случай сре...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>3_месяц_смена_момент_сделать</td>\n",
       "      <td>[месяц, смена, момент, сделать, год, время, за...</td>\n",
       "      <td>[идея внутрений резерв сотрудник учёт перезда ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>4_новый_палантир_схема_приходиться</td>\n",
       "      <td>[новый, палантир, схема, приходиться, предопла...</td>\n",
       "      <td>[новый схема оплата палантир честно кошмар кли...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>5_зарплата_условие_карьерный_большой</td>\n",
       "      <td>[зарплата, условие, карьерный, большой, рост, ...</td>\n",
       "      <td>[зарплата малый возможность рост ограничить по...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>6_попасть_технолог_старший технолог_алхимик</td>\n",
       "      <td>[попасть, технолог, старший технолог, алхимик,...</td>\n",
       "      <td>[высокий уровень программа стажировка молодой ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>34</td>\n",
       "      <td>7_ребёнок_квартал_премия_прошлый</td>\n",
       "      <td>[ребёнок, квартал, премия, прошлый, год, пособ...</td>\n",
       "      <td>[прошлый квартал премия получить змз белый гор...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>8_праздник_день рождение_рождение_день</td>\n",
       "      <td>[праздник, день рождение, рождение, день, скат...</td>\n",
       "      <td>[ранний начать открывать простой колег друг др...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>9_друг_атмосфера_коллега_семья</td>\n",
       "      <td>[друг, атмосфера, коллега, семья, друг друг, к...</td>\n",
       "      <td>[ранний начинаться атмосфера агресивень рватьс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>10_добираться_трас_транспорт_ехать</td>\n",
       "      <td>[добираться, трас, транспорт, ехать, ездить, о...</td>\n",
       "      <td>[расположить трас ао ближьий население рункт д...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>11_мёд_томлёный_продажа_продажа топлёный мёд</td>\n",
       "      <td>[мёд, томлёный, продажа, продажа топлёный мёд,...</td>\n",
       "      <td>[продажа топлёный мёд точка смс взлететь огнен...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                          Name  \\\n",
       "0      -1    449                 -1_новый_стать_время_например   \n",
       "1       0     69                0_скатя_снежный_зеркало_словно   \n",
       "2       1     62                    1_день_смена_ночной_делать   \n",
       "3       2     61               2_знать_образование_делать_опыт   \n",
       "4       3     54                  3_месяц_смена_момент_сделать   \n",
       "5       4     41            4_новый_палантир_схема_приходиться   \n",
       "6       5     40          5_зарплата_условие_карьерный_большой   \n",
       "7       6     40   6_попасть_технолог_старший технолог_алхимик   \n",
       "8       7     34              7_ребёнок_квартал_премия_прошлый   \n",
       "9       8     31        8_праздник_день рождение_рождение_день   \n",
       "10      9     30                9_друг_атмосфера_коллега_семья   \n",
       "11     10     26            10_добираться_трас_транспорт_ехать   \n",
       "12     11     22  11_мёд_томлёный_продажа_продажа топлёный мёд   \n",
       "\n",
       "                                       Representation  \\\n",
       "0   [новый, стать, время, например, проблема, фени...   \n",
       "1   [скатя, снежный, зеркало, словно, дракон, испо...   \n",
       "2   [день, смена, ночной, делать, график, стоить, ...   \n",
       "3   [знать, образование, делать, опыт, идея, безоп...   \n",
       "4   [месяц, смена, момент, сделать, год, время, за...   \n",
       "5   [новый, палантир, схема, приходиться, предопла...   \n",
       "6   [зарплата, условие, карьерный, большой, рост, ...   \n",
       "7   [попасть, технолог, старший технолог, алхимик,...   \n",
       "8   [ребёнок, квартал, премия, прошлый, год, пособ...   \n",
       "9   [праздник, день рождение, рождение, день, скат...   \n",
       "10  [друг, атмосфера, коллега, семья, друг друг, к...   \n",
       "11  [добираться, трас, транспорт, ехать, ездить, о...   \n",
       "12  [мёд, томлёный, продажа, продажа топлёный мёд,...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [цинтрийский феникс стараться обновление ценик...  \n",
       "1   [мантикорный трубка дело хорош перекачка эфект...  \n",
       "2   [растроить девушка буквально декабрь проситься...  \n",
       "3   [думать малый знание начинать знать случай сре...  \n",
       "4   [идея внутрений резерв сотрудник учёт перезда ...  \n",
       "5   [новый схема оплата палантир честно кошмар кли...  \n",
       "6   [зарплата малый возможность рост ограничить по...  \n",
       "7   [высокий уровень программа стажировка молодой ...  \n",
       "8   [прошлый квартал премия получить змз белый гор...  \n",
       "9   [ранний начать открывать простой колег друг др...  \n",
       "10  [ранний начинаться атмосфера агресивень рватьс...  \n",
       "11  [расположить трас ао ближьий население рункт д...  \n",
       "12  [продажа топлёный мёд точка смс взлететь огнен...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Если сгенерировались только 2-3 тематики стоит перезапустить алгоритм*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**КЛАССИФИКАЦИЯ**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Теперь, когда датасет размечен, можно реализовать модель классификации комментариев по тематикам.\n",
    "\n",
    "Из таблицы `get_topic_info` видно, что классы получились крайне несбалансированными: некоторые из них представлены выборками всего из 20 элементов, что затрудняет разбиение на тренировочную и тестовую выборки. \n",
    "\n",
    "Для решения этой проблемы использовал SMOTE (https://habr.com/ru/companies/otus/articles/782668/). Для построения эмбеддингов по-прежнему используется модель `'cointegrated/rubert-tiny2'`, которая работает хорошо, так зачем её менять? :)\n",
    "\n",
    "Изначально планировалось использовать *бейзлайн* модель `RandomForestClassifier` для классификации, однако модель смогла показать высокие результаты как по accuracy (>95%), так и по остальным метрикам (что видно ниже), поэтому было решено остановиться на ней.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              comments  topic\n",
      "0    среди ночь район час упасть полха вода стеклян...     -1\n",
      "1    програм повышение квалификаци гильдия цмф хитр...      6\n",
      "2    мур новиград город контраст цмф змс человек пр...      9\n",
      "3    тип полгода ждать установка фильтр маслобаза д...     -1\n",
      "4    всё просто ранний бумажка летать теряться иска...     -1\n",
      "..                                                 ...    ...\n",
      "954  город знать ночной смена работать это нормальн...     -1\n",
      "955  праздник както както всё это очень организоват...      8\n",
      "956        женский коллектив всё равно склока избежать      9\n",
      "957  ещё нехват очень больший стоить наш помещение ...     -1\n",
      "958  мур сначала вроде хершо палантирчик всё подсчи...     -1\n",
      "\n",
      "[959 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comments = [' '.join(s) for s in processed_sentences]\n",
    "\n",
    "df = pd.DataFrame(topics, columns=['topic'])\n",
    "df.insert(0, 'comments', comments)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9768835616438356\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      0.73      0.84        97\n",
      "           0       0.92      0.99      0.96        97\n",
      "           1       0.98      1.00      0.99        85\n",
      "           2       0.97      1.00      0.98        91\n",
      "           3       0.96      1.00      0.98        97\n",
      "           4       1.00      1.00      1.00       101\n",
      "           5       0.96      1.00      0.98        77\n",
      "           6       0.98      1.00      0.99        95\n",
      "           7       1.00      1.00      1.00        91\n",
      "           8       0.97      1.00      0.99        72\n",
      "           9       0.99      1.00      0.99        84\n",
      "          10       1.00      1.00      1.00        82\n",
      "          11       0.99      1.00      0.99        99\n",
      "\n",
      "    accuracy                           0.98      1168\n",
      "   macro avg       0.98      0.98      0.98      1168\n",
      "weighted avg       0.98      0.98      0.98      1168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny2')\n",
    "model = AutoModel.from_pretrained('cointegrated/rubert-tiny2')\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128, verbose=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embedding\n",
    "\n",
    "embeddings = df['comments'].apply(get_embedding)\n",
    "X = list(embeddings)\n",
    "y = df['topic']\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ЗАКЛЮЧЕНИЕ**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "В заключение скажу, что выбранный подход к решению поставленной задачи отвечает всем заданным техническим и бизнес-требованиям и, по мнению автора, считается оптимальным с учетом приведенных ниже возможностей для улучшений и оптимизаций.\n",
    "\n",
    "Далее предложены варианты улучшения системы:\n",
    "\n",
    "1. Первый этап — предобработка текстовых данных.\n",
    "\n",
    "    Здесь я бы улучшил способ обработки грамматических ошибок в тексте, например, используя `Fine-tuned LLM`. Текст без ошибок лучше стандартизируется с помощью техник лемматизации, что, в свою очередь, улучшает качество работы моделей тематического моделирования. Также важно обрабатывать именованные сущности отдельно.\n",
    "\n",
    "2. Второй этап — тематическое моделирование.\n",
    "\n",
    "    Как я описывал ранее, качество работы алгоритмов тематического моделирования можно повысить за счет добавления эвристик, например списка потенциальных тематик. Также можно попробовать использовать более \"тяжелые\" модели для генерации эмбеддингов и протестировать большое количество параметров для подбора оптимальных значений.\n",
    "\n",
    "3. Третий этап — классификация.\n",
    "\n",
    "    Здесь есть значительный простор для улучшений, так как используется простая базовая модель. Как минимум, ее можно заменить на многослойный перцептрон, но я не уверен в необходимости этого.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
